"""
Time Series Transformer Model Implementation.

This module provides implementation for Time Series Transformer.
"""

import torch
import torch.nn as nn


class TimeSeriesTransformer(nn.Module):
    """
    A minimal causal Transformer model.

    The model is designed for learning the distribution of
    discrete token sequences generated by a VQ-VAE tokenizer for time series data.
    This model takes sequences of discrete codes (e.g., [c1, c2, c3, c4]) and learns
    an autoregressive prior p(c_t | c_1, ..., c_{t-1}) using causal self-attention.
    It can then be used to sample new token sequences, which are subsequently
    decoded back into continuous time series by the VQ-VAE decoder.
    """

    def __init__(
        self,
        n_codes: int = 64,
        n_tokens: int = 4,
        d_model: int = 32,
        n_heads: int = 2,
        n_layers: int = 2,
    ):
        """
        Initialize the TimeSeriesTransformer.

        Args:
            n_codes (int): The size of the discrete codebook from the VQ-VAE.
                           Determines the number of unique tokens (0 to n_codes-1).
                           Also defines the output size of the final linear layer.
                           Default is 64.
            n_tokens (int): The number of tokens representing each time series sequence
                            after VQ-VAE encoding. This defines the maximum sequence
                            length the model can handle and the size of positional embeddings.
                            Default is 4.
            d_model (int): The dimensionality of the token embeddings and the model's
                           internal representations. Default is 32.
            n_heads (int): The number of attention heads in the Transformer layers.
                           Default is 2.
            n_layers (int): The number of Transformer encoder layers stacked.
                            Default is 2.
        """
        super().__init__()
        self.n_tokens = n_tokens
        self.n_codes = n_codes

        # Token embedding + positional encoding
        # Token embedding layer: maps discrete token IDs (0 to n_codes-1) to dense vectors
        self.tok_emb = nn.Embedding(n_codes, d_model)
        # Positional embedding layer: maps position indices (0 to n_tokens-1) to dense vectors
        self.pos_emb = nn.Embedding(n_tokens, d_model)

        # Transformer layers
        self.layers = nn.ModuleList(
            [
                nn.TransformerEncoderLayer(
                    d_model=d_model,
                    nhead=n_heads,
                    dim_feedforward=d_model * 2,  # Feedforward hidden layer size
                    dropout=0.1,  # Dropout for regularization
                    batch_first=True,  # Expected input shape: [Batch, SeqLen, Features]
                )
                for _ in range(n_layers)
            ]
        )

        # Output head: maps final hidden states back to logits over the codebook
        self.head = nn.Linear(d_model, n_codes)

        # Causal mask: ensures that prediction for position i only depends on positions < i
        # Uses -inf to mask future positions in attention logits.
        # Shape: [n_tokens, n_tokens]
        # mask[i, j] = 0 if j <= i (allowed), -inf if j > i (masked)
        mask = torch.tril(
            torch.ones(n_tokens, n_tokens), diagonal=0
        )  # 1s below/equal diag, 0s above
        causal_mask = torch.where(mask == 1, 0.0, float("-inf"))
        self.register_buffer("mask", causal_mask)

    def forward(self, codes: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for training the autoregressive model.

        Computes logits for the next token in the sequence for each position,
        conditioned on all previous tokens (causal attention).

        Args:
            codes (torch.Tensor): Input tensor of discrete token IDs.
                                  Shape: [Batch_Size, Sequence_Length],
                                  where Sequence_Length <= n_tokens.
                                  Values should be integers in [0, n_codes).

        Returns:
            torch.Tensor: Output logits for the next token prediction.
                          Shape: [Batch_Size, Sequence_Length, n_codes].
                          These logits represent unnormalized log probabilities for
                          each of the n_codes possible next tokens at each position.
        """
        # codes: [B, L] where L <= n_tokens
        B, L = codes.shape

        # Embed tokens and positions
        tok_emb = self.tok_emb(codes)  # [B, L, d_model]
        pos_emb = self.pos_emb(torch.arange(L, device=codes.device))  # [L, d_model]
        x = tok_emb + pos_emb  # [B, L, d_model]

        # Apply causal mask
        mask = self.mask[:L, :L]  # [L, L]

        # Pass through transformer layers
        for layer in self.layers:
            x = layer(x, src_mask=mask)

        # Predict next token logits
        logits = self.head(x)  # [B, L, n_codes]
        return logits
